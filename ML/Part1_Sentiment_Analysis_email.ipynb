{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xk5IaSk4oCoN",
        "outputId": "60da9fed-0738-49a4-a390-6e42628e12fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x38_i54WmmXr",
        "outputId": "4a69bb30-fffa-42f7-faac-8fda257dc08b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                           id  \\\n",
            "0       EW/2016/page_50.pdf-2   \n",
            "1     ETR/2016/page_352.pdf-3   \n",
            "2      GS/2012/page_186.pdf-3   \n",
            "3      ADI/2011/page_83.pdf-2   \n",
            "4     PNC/2014/page_203.pdf-3   \n",
            "...                       ...   \n",
            "1967    GS/2015/page_76.pdf-3   \n",
            "1968  JPM/2009/page_238.pdf-2   \n",
            "1969   AES/2001/page_45.pdf-2   \n",
            "1970  ETR/2017/page_175.pdf-3   \n",
            "1971   STT/2006/page_96.pdf-4   \n",
            "\n",
            "                                               pre_text predicted_sentiment  \n",
            "0     ['net cash flows provided by operating activit...             default  \n",
            "1     ['entergy louisiana , llc and subsidiaries man...             default  \n",
            "2     ['notes to consolidated financial statements n...             enquiry  \n",
            "3     ['the following is a schedule of future minimu...             default  \n",
            "4     ['to determine stock-based compensation expens...             default  \n",
            "...                                                 ...                 ...  \n",
            "1967  ['the goldman sachs group , inc .'\\n 'and subs...             default  \n",
            "1968  ['notes to consolidated financial statements j...             default  \n",
            "1969  ['of sales , competitive supply gross margin d...             default  \n",
            "1970  ['as of december a031 , 2017 , system energy ,...             default  \n",
            "1971  ['for the year ended december 31 , 2005 , we r...        dissatisfied  \n",
            "\n",
            "[1972 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "# Install NLTK and download required resources\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Load the dataset from CSV\n",
        "dataset_path = '/content/Train - Email Classification.csv'  # Replace with your CSV file path\n",
        "dataset = pd.read_csv(dataset_path)\n",
        "\n",
        "# Sample email text\n",
        "email_texts = dataset['pre_text']\n",
        "\n",
        "# Define keyword dictionaries for different sentiment categories\n",
        "sentiment_keywords = {\n",
        "    'enquiry': ['question', 'query', 'enquiry', 'information'],\n",
        "    'dissatisfied': ['dissatisfied', 'unhappy', 'unsatisfied', 'complaint', 'issue','mess'],\n",
        "    'Satisfied': ['Satisfied', 'happy', 'pleased', 'good', 'great']\n",
        "    'Emergency': ['Emergency', 'Urgent', 'important', 'crisis', 'great','disaster','ASAP'],\n",
        "}\n",
        "\n",
        "# Perform sentiment analysis for each email text\n",
        "predicted_sentiments = []\n",
        "\n",
        "for email_text in email_texts:\n",
        "    # Tokenize the email text\n",
        "    tokens = word_tokenize(email_text.lower())\n",
        "\n",
        "    # Check for keywords in each sentiment category\n",
        "    category_scores = {category: sum(1 for word in tokens if word in keywords) for category, keywords in sentiment_keywords.items()}\n",
        "\n",
        "    # Determine the category with the highest score\n",
        "    max_category = max(category_scores, key=category_scores.get)\n",
        "    max_score = category_scores[max_category]\n",
        "\n",
        "    # Thresholding: If no keywords are found or scores are tied, assign to a default category\n",
        "    if max_score == 0 or sum(score == max_score for score in category_scores.values()) > 1:\n",
        "        sentiment_category = 'default'\n",
        "    else:\n",
        "        sentiment_category = max_category\n",
        "\n",
        "    predicted_sentiments.append(sentiment_category)\n",
        "\n",
        "# Add the predicted sentiments to the dataset\n",
        "dataset['predicted_sentiment'] = predicted_sentiments\n",
        "\n",
        "# Display the dataset with predicted sentiments\n",
        "print(dataset[['id', 'pre_text', 'predicted_sentiment']])\n"
      ]
    }
  ]
}
